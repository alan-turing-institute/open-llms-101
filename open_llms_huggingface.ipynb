{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a LLM for Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a few initial command useful when setting up things on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only when running on Google Colab\n",
    "# !pip install datasets\n",
    "# import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#!git clone https://github.com/alan-turing-institute/open-llms-101.git\n",
    "#!mv open-llms-101/R1-Distill-SFT-sample ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model and a dataset of reasoning examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"PleIAs/Pleias-1.2b-Preview\" # or \"google/gemma-2-2b-it\" if you already have an huggingface account and have access to the model\n",
    "\n",
    "trial = True\n",
    "dataset_percentage = None if trial else 0.002 # Change to 100 to use the whole dataset\n",
    "\n",
    "if trial:\n",
    "    dataset = load_from_disk(\"R1-Distill-SFT-sample\")\n",
    "else:\n",
    "    dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", \"v1\", split=\"train\")\n",
    "    num_examples = int(len(dataset) * dataset_percentage / 100)\n",
    "\n",
    "    # Select the first `num_examples` from the dataset\n",
    "    dataset = dataset.select(range(num_examples))\n",
    "\n",
    "print (f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Check if pad_token is set; if not, set it\n",
    "if tokenizer.pad_token is None:\n",
    "    print (\"Pad token not set; setting to '[PAD]'\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})# Check if pad_token is set; if not, set it\n",
    "    # Resize model embeddings to match the tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for \"chat\" abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, prompt, length, attention_mask=None, pad_token_id=None):\n",
    "    # Prepare the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "    if pad_token_id is None:\n",
    "        pad_token_id = tokenizer.eos_token_id  # Explicitly set pad_token_id\n",
    "\n",
    "    # Set up the streamer\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "    # Generate tokens in a separate thread to allow streaming\n",
    "    generation_kwargs = dict(\n",
    "        inputs=inputs.input_ids,\n",
    "        attention_mask=attention_mask,  # Explicitly pass attention_mask\n",
    "        max_new_tokens=length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=pad_token_id,  # Explicitly pass pad_token_id\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    # Start generation in a new thread\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Stream output token by token\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "\n",
    "    # Wait for the thread to finish\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing generating content abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a chat session\n",
    "prompt = \"What is AIUK?\"\n",
    "chat(model, prompt, length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the reasoning dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Function to format examples\n",
    "def format_example(example):\n",
    "    messages = example[\"reannotated_messages\"]    \n",
    "    # Extract user input (prompt)\n",
    "    user_input = next(msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\")\n",
    "    \n",
    "    # Extract assistant response (reasoning + final answer)\n",
    "    assistant_response = \" \".join(msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\")\n",
    "    assistant_response = assistant_response.replace(\"</think>\", \"</think><answer>\") + \"</answer>\"\n",
    "    return {\"text\": \"<prompt>\"+ user_input + \"</prompt>\" + assistant_response}\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup fine-tuning and start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"trained_models/{dataset_percentage}_fine_tuned_{model_name.split('/')[-1]}\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File {filename} already exists, skipping fine-tuning\")\n",
    "\n",
    "else:\n",
    "    # Process examples in parallel\n",
    "    dataset = dataset.map(format_example, num_proc=4)\n",
    "\n",
    "    # Tokenize efficiently\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=False, num_proc=4)\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,  # Rank\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Define data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"trained_models/results\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        logging_dir=\"trained_models/logs\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    peft_model.save_pretrained(filename)\n",
    "    tokenizer.save_pretrained(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reasoning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load PEFT adapter\n",
    "reload_peft_model = PeftModel.from_pretrained(model, filename)\n",
    "\n",
    "# Prepare the input prompt\n",
    "prompt = \"<prompt>Alice, Bob, and Charlie are in a room. Alice always tells the truth, Bob always lies, and Charlie sometimes lies and sometimes tells the truth. You ask each of them, ‘Is Charlie a truth-teller?’ Alice says, ‘No.’ Bob says, ‘Yes.’ Charlie says, ‘I sometimes lie.’ Who is telling the truth?</prompt>\"\n",
    "\n",
    "# Generate text\n",
    "chat(reload_peft_model, prompt, length=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
