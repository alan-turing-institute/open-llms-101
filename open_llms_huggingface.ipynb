{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only when running on Google Colab\n",
    "# !pip install datasets\n",
    "#import os\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/gemma-2-2b-it\" # or \"google/gemma-2-2b-it\" if you already have an huggingface account and have access to the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Check if pad_token is set; if not, set it\n",
    "if tokenizer.pad_token is None:\n",
    "    print (\"Pad token not set; setting to '[PAD]'\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})# Check if pad_token is set; if not, set it\n",
    "    # Resize model embeddings to match the tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, prompt, length, attention_mask=None, pad_token_id=None):\n",
    "    # Prepare the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    if attention_mask is None:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "    if pad_token_id is None:\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Set up the streamer\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)\n",
    "\n",
    "    # Generate tokens in a separate thread to allow streaming\n",
    "    generation_kwargs = dict(\n",
    "        inputs=inputs.input_ids,\n",
    "        max_new_tokens=length,  # Increase this for longer output\n",
    "        temperature=0.7,\n",
    "        do_sample=True,  # Allows creative output\n",
    "        top_k=50,        # Limits sampling to top 50 tokens\n",
    "        top_p=0.95,      # Nucleus sampling for diversity`\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    # Start generation in a new thread\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Stream output token by token\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "\n",
    "    # Wait for the thread to finish\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a chat session\n",
    "prompt = \"What is AIUK?\"\n",
    "chat(model, prompt, length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning for Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Function to format examples\n",
    "def format_example(example):\n",
    "    messages = example[\"reannotated_messages\"]    \n",
    "    # Extract user input (prompt)\n",
    "    user_input = next(msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\")\n",
    "    \n",
    "    # Extract assistant response (reasoning + final answer)\n",
    "    assistant_response = \" \".join(msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\")\n",
    "    assistant_response = assistant_response.replace(\"</think>\", \"</think><answer>\") + \"</answer>\"\n",
    "    return {\"text\": \"<prompt>\"+ user_input + \"</prompt>\" + assistant_response}\n",
    "\n",
    "# Load full dataset first (just for length calculation)\n",
    "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", \"v1\", split=\"train\")\n",
    "\n",
    "# Calculate number of examples to keep \n",
    "percentage = 0.0001\n",
    "\n",
    "filename = f\"trained_models/{percentage}_fine_tuned_{model_name.split('/')[-1]}\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File {filename} already exists, skipping fine-tuning\")\n",
    "\n",
    "else:\n",
    "    num_examples = int(len(dataset) * percentage / 100)\n",
    "\n",
    "    # Select the first `num_examples` from the dataset\n",
    "    dataset = dataset.select(range(num_examples))\n",
    "\n",
    "    # Continue with formatting and processing as before\n",
    "    dataset = dataset.map(format_example, num_proc=4)\n",
    "\n",
    "    # Process examples in parallel\n",
    "    dataset = dataset.map(format_example, num_proc=4)\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,  # Rank\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Tokenize efficiently\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=False, num_proc=4)\n",
    "\n",
    "    # Define data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"trained_models/results\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    peft_model.save_pretrained(filename)\n",
    "    tokenizer.save_pretrained(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load PEFT adapter\n",
    "reload_peft_model = PeftModel.from_pretrained(model, filename)\n",
    "\n",
    "# Prepare the input prompt\n",
    "prompt = \"<prompt>Alice, Bob, and Charlie are in a room. Alice always tells the truth, Bob always lies, and Charlie sometimes lies and sometimes tells the truth. You ask each of them, ‘Is Charlie a truth-teller?’ Alice says, ‘No.’ Bob says, ‘Yes.’ Charlie says, ‘I sometimes lie.’ Who is telling the truth?</prompt>\"\n",
    "\n",
    "# Generate text\n",
    "chat(reload_peft_model, prompt, length=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
