{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input prompt\n",
    "prompt = \"What do you think of AIUK?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")  # Use \"cpu\" if no GPU\n",
    "\n",
    "# Set up the streamer\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Generate tokens in a separate thread to allow streaming\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    max_new_tokens=100,  # Increase this for longer output\n",
    "    temperature=0.7,\n",
    "    do_sample=True,  # Allows creative output\n",
    "    top_k=50,        # Limits sampling to top 50 tokens\n",
    "    top_p=0.95,      # Nucleus sampling for diversity`\n",
    "    streamer=streamer\n",
    ")\n",
    "\n",
    "# Start generation in a new thread\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# Print the prompt\n",
    "print(prompt, end=\"\", flush=True)\n",
    "\n",
    "# Stream output token by token\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "# Wait for the thread to finish\n",
    "thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
