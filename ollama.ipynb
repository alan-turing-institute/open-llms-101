{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step process\n",
    "\n",
    "Download, install and starts [Ollama](https://ollama.com/)\n",
    "\n",
    "Run `ollama run gemma2:2b`\n",
    "\n",
    "Start chatting with a Gemma model that runs directly on your laptop!\n",
    "\n",
    "Then you can explore many more models here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whether or not Ollama is the **best** tool you've ever encountered depends entirely on your needs and what you consider \"best.\" \n",
       "\n",
       "Here's why:\n",
       "\n",
       "**Ollama shines in these areas:**\n",
       "\n",
       "* **Accessibility and affordability:** Being open-source and freely available makes it accessible to a wide range of users, especially those with limited budgets.\n",
       "* **Flexibility and customization:** Ollama allows for fine-tuning and adaptation to specific tasks and use cases, making it a versatile tool for various projects.\n",
       "* **Ease of use:**  Ollama offers a user-friendly interface and requires minimal technical expertise to get started.\n",
       "\n",
       "**However, there are some limitations to consider:**\n",
       "\n",
       "* **Resource requirements:** While Ollama is relatively lightweight compared to other LLMs, it still demands considerable computational power and resources for optimal performance.\n",
       "* **Fine-tuning challenges:**  Ollama's flexibility comes with a caveat – fine-tuning can be complex and time-consuming depending on the desired output and dataset. \n",
       "* **Limited knowledge scope:**  While Ollama boasts impressive capabilities, it is still under development and its knowledge base might not encompass all the information necessary for complex tasks.\n",
       "\n",
       "**What makes \"best\" subjective:**\n",
       "\n",
       "The best tool depends on your specific context:\n",
       "\n",
       "* **For research and experimentation:** Ollama's open-source nature and ease of customization make it a strong contender for exploring the potential of LLMs.\n",
       "* **For text generation and summarization:** Its capabilities in these areas are impressive, especially for user-friendly applications like chatbot development. \n",
       "* **For complex tasks requiring specific domain expertise:**  Ollama might be less suitable compared to specialized LLMs focused on particular fields like healthcare or legal jargon.\n",
       "\n",
       "**Ultimately,** Ollama is a powerful tool with its own strengths and limitations. To determine if it's the \"best\" for *you*, you need to:\n",
       "\n",
       "1. **Define your needs:**  What tasks do you want to accomplish? What kind of expertise do you require in the tool?\n",
       "2. **Compare Ollama to alternatives:** Research other tools available, considering their strengths and weaknesses relative to your specific requirements. \n",
       "\n",
       "\n",
       "Instead of seeking the absolute \"best,\" focus on finding the **most suitable** tool for your unique needs and goals. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def get_response_from_model(prompt, model):\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "    return response['response']\n",
    "\n",
    "# Define the prompt and model\n",
    "prompt = \"Is Ollama the best tool I have ever encountered?\"\n",
    "model = \"gemma2:2b\"\n",
    "\n",
    "# Get the response from the model\n",
    "response = get_response_from_model(prompt, model)\n",
    "\n",
    "# Display the Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retieval augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ollama pull mxbai-embed-large\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embed(model=\"mxbai-embed-large\", input=d)\n",
    "  embeddings = response[\"embeddings\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=embeddings,\n",
    "    documents=[d]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example input\n",
    "input = \"What animals are llamas related to?\"\n",
    "\n",
    "# generate an embedding for the input and retrieve the most relevant doc\n",
    "response = ollama.embed(\n",
    "  model=\"mxbai-embed-large\",\n",
    "  input=input\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embeddings\"][0]],\n",
    "  n_results=1\n",
    ")\n",
    "data = results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are related to **vicuñas** and **camels**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model=\"gemma2:2b\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {input}\"\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More realistic scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the ag_news dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")  # Using the first 1000 articles for example\n",
    "\n",
    "# Sample documents from the dataset (for simplicity, use the 'text' field)\n",
    "documents = dataset['text']\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"news\")\n",
    "\n",
    "# Store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embed(model=\"mxbai-embed-large\", input=d)\n",
    "  embeddings = response[\"embeddings\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=embeddings,\n",
    "    documents=[d]\n",
    "  )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
